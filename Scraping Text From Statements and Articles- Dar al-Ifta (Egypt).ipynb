{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In a previous notebook we crawled through the Dar's webpages and got the links for all statements and articles. \n",
    "\n",
    "#however, we need to get the texts of those statements and articles and we can use the CSVs we created to get them. \n",
    "\n",
    "\n",
    "\n",
    "#We will use beautiful soup for this part. \n",
    "\n",
    "import csv\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#This code unspools the CSV in which we saved all the statements for the links. \n",
    "\n",
    "statement_links = []\n",
    "with open('dar_statements_2.csv') as csv_file:\n",
    "    statement_reader = csv.reader(csv_file, delimiter = ',')\n",
    "    for row in statement_reader:\n",
    "        for thing in row:\n",
    "            statement_links.append(thing)\n",
    "\n",
    "all_statements = []\n",
    "dead_link = []\n",
    "            \n",
    "for statement in statement_links:\n",
    "    print(statement)\n",
    "    article_content = []\n",
    "    url = requests.get(statement)\n",
    "    soup = BeautifulSoup(url.content, 'lxml')\n",
    "    article = soup.find(\"td\", {\"class\" : \"lblFatwaItem\"})\n",
    "    if article == None: \n",
    "        dead_link.append(statement)\n",
    "    else: \n",
    "        text = article.find_all(\"p\")\n",
    "        whole_article = \"\"\n",
    "        for paragraph in text:\n",
    "            words = paragraph.text + \" \"\n",
    "            whole_article += words\n",
    "        link = statement\n",
    "        title = soup.find('h2', {\"class\" : \"contentheading clearfix\"})\n",
    "        real_title = title.text\n",
    "        classification = \"statement\"\n",
    "        date_start = whole_article.rfind(\"المركز الإعلامي بدار الإفتاء المصرية\") \n",
    "        date = whole_article[date_start + 36:]\n",
    "        date = date.strip()\n",
    "        #There is some variation in how dates are writtin on the website. \n",
    "        #In order to account for when they are written out long hand (Feburary 2, XXXX)\n",
    "        #I created this bit of code to put in my birthday so I can manually fix it\n",
    "        if len(date) > 12:\n",
    "            date = \"03-03-1989\"\n",
    "        date = date.replace(\"٠\", \"0\")\n",
    "        date = date.replace (\"١\", \"1\")\n",
    "        date = date.replace(\"٢\", \"2\")\n",
    "        date = date.replace(\"٣\", \"3\")\n",
    "        date = date.replace(\"٤\", \"4\")\n",
    "        date = date.replace(\"٥\", \"5\")\n",
    "        date = date.replace(\"٦\", \"6\")\n",
    "        date = date.replace(\"٧\", \"7\")\n",
    "        date = date.replace(\"٨\", \"8\")\n",
    "        date = date.replace(\"٩\", \"9\")\n",
    "        date = date.replace(\"م\", \"\")\n",
    "        movable_date = date.split(\"-\")\n",
    "        #Some dates are saved with slashes rather than -, this code captures those\n",
    "        if len(movable_date) == 1:\n",
    "            movable_date = movable_date[0]\n",
    "            movable_date = movable_date.split(\"/\")\n",
    "        #Some articles have titles and no texts and dates, this code gives those dates no values. \n",
    "        if movable_date[0] == \"\":\n",
    "            movable_date = [None] * 3\n",
    "        year = movable_date[0]\n",
    "        month = movable_date[1]\n",
    "        day = movable_date[2]\n",
    "        article_content.append(link)\n",
    "        article_content.append(real_title)\n",
    "        article_content.append(whole_article)\n",
    "        article_content.append(date)\n",
    "        article_content.append(year)\n",
    "        article_content.append(month)\n",
    "        article_content.append(day)\n",
    "        article_content.append(classification)\n",
    "        all_statements.append(article_content)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "place_holder = all_statements\n",
    "\n",
    "new_all_statements = []\n",
    "\n",
    "for statement in place_holder:\n",
    "    statement[1] = statement[1].replace('\\n', '')\n",
    "    statement[1] = statement[1].replace('\\t', '')\n",
    "    statement[1] = statement[1].replace('\\r', \"\")\n",
    "\n",
    "    statement[2] = statement[2].replace('\\n', '')\n",
    "    statement[2] = statement[2].replace('\\t', \"\")\n",
    "    statement[2] = statement[2].replace(\"\\r\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can then save the revised list here: \n",
    "\n",
    "\n",
    "\n",
    "with open(\"new_dar_statements_text.csv\", mode = \"w\", encoding = \"utf-8\") as dar_file:\n",
    "    dar_writer = csv.writer(dar_file)\n",
    "    \n",
    "    dar_writer.writerow([\"link\", 'title', 'text', 'date', 'year', 'month', 'day', 'category'])\n",
    "    for statement in new_all_statements:\n",
    "        dar_writer.writerow(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Articles \n",
    "\n",
    "import csv\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "article_links = []\n",
    "with open('dar_articles.csv') as csv_file:\n",
    "    article_reader = csv.reader(csv_file, delimiter = ',')\n",
    "    for row in article_reader:\n",
    "        for thing in row:\n",
    "            article_links.append(thing)\n",
    "            \n",
    "            \n",
    "all_articles = []\n",
    "dead_link = []\n",
    "            \n",
    "for statement in article_links:\n",
    "    print(statement)\n",
    "    article_content = []\n",
    "    url = requests.get(statement)\n",
    "    soup = BeautifulSoup(url.content, 'lxml')\n",
    "    article = soup.find(\"td\", {\"class\" : \"lblFatwaItem\"})\n",
    "    if article == None: \n",
    "        dead_link.append(statement)\n",
    "    else: \n",
    "        text = article.find_all(\"p\")\n",
    "        whole_article = \"\"\n",
    "        for paragraph in text:\n",
    "            words = paragraph.text + \" \"\n",
    "            whole_article += words\n",
    "        whole_article = whole_article.strip(\"\\n\")\n",
    "        whole_article = whole_article.strip(\"\\t\")\n",
    "        whole_article = whole_article.strip('\\r')\n",
    "        link = statement\n",
    "        title = soup.find('h2', {\"class\" : \"contentheading clearfix\"})\n",
    "        real_title = title.text\n",
    "        classification = \"article\"\n",
    "        date_start = whole_article.rfind(\"المركز الإعلامي بدار الإفتاء المصرية\") \n",
    "        date = whole_article[date_start + 36:]\n",
    "        date = date.strip()\n",
    "        if len(date) > 12:\n",
    "            date = \"03-03-1989\"\n",
    "        date = date.replace(\"٠\", \"0\")\n",
    "        date = date.replace (\"١\", \"1\")\n",
    "        date = date.replace(\"٢\", \"2\")\n",
    "        date = date.replace(\"٣\", \"3\")\n",
    "        date = date.replace(\"٤\", \"4\")\n",
    "        date = date.replace(\"٥\", \"5\")\n",
    "        date = date.replace(\"٦\", \"6\")\n",
    "        date = date.replace(\"٧\", \"7\")\n",
    "        date = date.replace(\"٨\", \"8\")\n",
    "        date = date.replace(\"٩\", \"9\")\n",
    "        date = date.replace(\"م\", \"\")\n",
    "        movable_date = date.split(\"-\")\n",
    "        if len(movable_date) == 1:\n",
    "            movable_date = movable_date[0]\n",
    "            movable_date = movable_date.split(\"/\")\n",
    "        if movable_date[0] == \"\":\n",
    "            movable_date = [None] * 3\n",
    "        year = movable_date[0]\n",
    "        month = movable_date[1]\n",
    "        day = movable_date[2]\n",
    "        article_content.append(link)\n",
    "        article_content.append(real_title)\n",
    "        article_content.append(whole_article)\n",
    "        article_content.append(date)\n",
    "        article_content.append(year)\n",
    "        article_content.append(month)\n",
    "        article_content.append(day)\n",
    "        article_content.append(classification)\n",
    "        all_articles.append(article_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the articles in a CSV\n",
    "\n",
    "\n",
    "with open(\"dar_articles_text.csv\", mode = \"w\", encoding = \"utf-8\") as dar_file:\n",
    "    dar_writer = csv.writer(dar_file)\n",
    "    \n",
    "    dar_writer.writerow([\"link\", 'title', 'text', 'date', 'day', 'month', 'year', 'category'])\n",
    "    for article in place_holder:\n",
    "        dar_writer.writerow(article)\n",
    "\n",
    "dar_file.close()        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
